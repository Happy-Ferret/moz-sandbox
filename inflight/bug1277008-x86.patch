# HG changeset patch
# User Lars T Hansen <lhansen@mozilla.com>
# Date 1466240375 -7200
#      Sat Jun 18 10:59:35 2016 +0200
# Node ID becbb7062789e6127369bb9b6e58960985057ef8
# Parent  7ad5763a7404c420b93301f17ea2e9bb66e5f00b
Bug 1277008 - x86 support.

diff --git a/js/src/asmjs/WasmBaselineCompile.cpp b/js/src/asmjs/WasmBaselineCompile.cpp
--- a/js/src/asmjs/WasmBaselineCompile.cpp
+++ b/js/src/asmjs/WasmBaselineCompile.cpp
@@ -24,23 +24,22 @@
  *
  * Unimplemented functionality:
  *
  *  - This is not actually a baseline compiler, as it performs no
  *    profiling and does not trigger ion compilation and function
  *    replacement (duh)
  *  - int64 load and store
  *  - SIMD
- *  - Atomics
+ *  - Atomics (very simple now, we have range checking)
  *  - current_memory, grow_memory
  *  - non-signaling interrupts
  *  - non-signaling bounds checks
  *  - profiler support (devtools)
  *  - Platform support:
- *      x86
  *      ARM-32
  *      ARM-64
  *
  * There are lots of machine dependencies here but they are pretty
  * well isolated to a segment of the compiler.  Many dependencies
  * will eventually be factored into the MacroAssembler layer and shared
  * with other code generators.
  *
@@ -160,16 +159,19 @@ static const Register StackPointer = Rea
 
 #ifdef JS_CODEGEN_X86
 // The selection of EBX here steps gingerly around: the need for EDX
 // to be allocatable for multiply/divide; ECX to be allocatable for
 // shift/rotate; EAX (= ReturnReg) to be allocatable as the joinreg;
 // EBX not being one of the WasmTableCall registers; and needing a
 // temp register for load/store that has a single-byte persona.
 static const Register ScratchRegX86 = ebx;
+
+static const AllocatableGeneralRegisterSet SingleByteRegs(GeneralRegisterSet(
+                                                              Registers::SingleByteRegs));
 #endif
 
 class BaseCompiler
 {
     // We define our own ScratchRegister abstractions, deferring to
     // the platform's when possible.
 
 #if defined(JS_CODEGEN_X64) || defined(JS_CODEGEN_X86) || defined(JS_CODEGEN_ARM)
@@ -299,20 +301,49 @@ class BaseCompiler
     struct AnyReg
     {
         AnyReg() { tag = NONE; }
         explicit AnyReg(RegI32 r) { tag = I32; i32_ = r; }
         explicit AnyReg(RegI64 r) { tag = I64; i64_ = r; }
         explicit AnyReg(RegF32 r) { tag = F32; f32_ = r; }
         explicit AnyReg(RegF64 r) { tag = F64; f64_ = r; }
 
-        RegI32 i32() { MOZ_ASSERT(tag == I32); return i32_; }
-        RegI64 i64() { MOZ_ASSERT(tag == I64); return i64_; }
-        RegF32 f32() { MOZ_ASSERT(tag == F32); return f32_; }
-        RegF64 f64() { MOZ_ASSERT(tag == F64); return f64_; }
+        RegI32 i32() {
+            MOZ_ASSERT(tag == I32);
+            return i32_;
+        }
+        RegI64 i64() {
+            MOZ_ASSERT(tag == I64);
+            return i64_;
+        }
+        RegF32 f32() {
+            MOZ_ASSERT(tag == F32);
+            return f32_;
+        }
+        RegF64 f64() {
+            MOZ_ASSERT(tag == F64);
+            return f64_;
+        }
+        AnyRegister any() {
+            switch (tag) {
+              case F32: return AnyRegister(f32_.reg);
+              case F64: return AnyRegister(f64_.reg);
+              case I32: return AnyRegister(i32_.reg);
+              case I64:
+#ifdef JS_PUNBOX64
+                return AnyRegister(i64_.reg.reg);
+#else
+                MOZ_CRASH("WasmBaseline platform hook: AnyReg::any()");
+#endif
+              case NONE:
+                MOZ_CRASH("AnyReg::any() on NONE");
+            }
+            // Work around GCC 5 analysis/warning bug.
+            MOZ_CRASH("AnyReg::any(): impossible case");
+        }
 
         union {
             RegI32 i32_;
             RegI64 i64_;
             RegF32 f32_;
             RegF64 f64_;
         };
         enum { NONE, I32, I64, F32, F64 } tag;
@@ -1678,16 +1709,19 @@ class BaseCompiler
     }
 
     Control& controlItem(uint32_t relativeDepth) {
         return ctl_[ctl_.length() - 1 - relativeDepth];
     }
 
     MOZ_MUST_USE
     PooledLabel* newLabel() {
+        // TODO / INVESTIGATE: allocate() is fallible, but we can
+        // probably rely on an infallible allocator here.  That would
+        // simplify code later.
         PooledLabel* candidate = labelPool_.allocate();
         if (!candidate)
             return nullptr;
         return new (candidate) PooledLabel(this);
     }
 
     void freeLabel(PooledLabel* label) {
         label->~PooledLabel();
@@ -2614,217 +2648,470 @@ class BaseCompiler
         masm.breakpoint();
 #endif
     }
 
     //////////////////////////////////////////////////////////////////////
     //
     // Global variable access.
 
-    // CodeGeneratorX64::visitAsmJSLoadGlobalVar()
+    // CodeGenerator{X86,X64}::visitAsmJSLoadGlobalVar()
 
     void loadGlobalVarI32(unsigned globalDataOffset, RegI32 r)
     {
-#ifdef JS_CODEGEN_X64
+#if defined(JS_CODEGEN_X64)
         CodeOffset label = masm.loadRipRelativeInt32(r.reg);
         masm.append(AsmJSGlobalAccess(label, globalDataOffset));
+#elif defined(JS_CODEGEN_X86)
+        CodeOffset label = masm.movlWithPatch(PatchedAbsoluteAddress(), r.reg);
+        masm.append(AsmJSGlobalAccess(label, globalDataOffset));
 #else
         MOZ_CRASH("BaseCompiler platform hook: loadGlobalVarI32");
 #endif
     }
 
     void loadGlobalVarI64(unsigned globalDataOffset, RegI64 r)
     {
-#ifdef JS_CODEGEN_X64
+#if defined(JS_CODEGEN_X64)
         CodeOffset label = masm.loadRipRelativeInt64(r.reg.reg);
         masm.append(AsmJSGlobalAccess(label, globalDataOffset));
 #else
         MOZ_CRASH("BaseCompiler platform hook: loadGlobalVarI64");
 #endif
     }
 
     void loadGlobalVarF32(unsigned globalDataOffset, RegF32 r)
     {
-#ifdef JS_CODEGEN_X64
+#if defined(JS_CODEGEN_X64)
         CodeOffset label = masm.loadRipRelativeFloat32(r.reg);
         masm.append(AsmJSGlobalAccess(label, globalDataOffset));
+#elif defined(JS_CODEGEN_X86)
+        CodeOffset label = masm.vmovssWithPatch(PatchedAbsoluteAddress(), r.reg);
+        masm.append(AsmJSGlobalAccess(label, globalDataOffset));
 #else
         MOZ_CRASH("BaseCompiler platform hook: loadGlobalVarF32");
 #endif
     }
 
     void loadGlobalVarF64(unsigned globalDataOffset, RegF64 r)
     {
-#ifdef JS_CODEGEN_X64
+#if defined(JS_CODEGEN_X64)
         CodeOffset label = masm.loadRipRelativeDouble(r.reg);
         masm.append(AsmJSGlobalAccess(label, globalDataOffset));
+#elif defined(JS_CODEGEN_X86)
+        CodeOffset label = masm.vmovsdWithPatch(PatchedAbsoluteAddress(), r.reg);
+        masm.append(AsmJSGlobalAccess(label, globalDataOffset));
 #else
         MOZ_CRASH("BaseCompiler platform hook: loadGlobalVarF32");
 #endif
     }
 
     // CodeGeneratorX64::visitAsmJSStoreGlobalVar()
 
     void storeGlobalVarI32(unsigned globalDataOffset, RegI32 r)
     {
-#ifdef JS_CODEGEN_X64
+#if defined(JS_CODEGEN_X64)
         CodeOffset label = masm.storeRipRelativeInt32(r.reg);
         masm.append(AsmJSGlobalAccess(label, globalDataOffset));
+#elif defined(JS_CODEGEN_X86)
+        CodeOffset label = masm.movlWithPatch(r.reg, PatchedAbsoluteAddress());
+        masm.append(AsmJSGlobalAccess(label, globalDataOffset));
 #else
         MOZ_CRASH("BaseCompiler platform hook: storeGlobalVarI32");
 #endif
     }
 
     void storeGlobalVarI64(unsigned globalDataOffset, RegI64 r)
     {
-#ifdef JS_CODEGEN_X64
+#if defined(JS_CODEGEN_X64)
         CodeOffset label = masm.storeRipRelativeInt64(r.reg.reg);
         masm.append(AsmJSGlobalAccess(label, globalDataOffset));
 #else
         MOZ_CRASH("BaseCompiler platform hook: storeGlobalVarI64");
 #endif
     }
 
     void storeGlobalVarF32(unsigned globalDataOffset, RegF32 r)
     {
-#ifdef JS_CODEGEN_X64
+#if defined(JS_CODEGEN_X64)
         CodeOffset label = masm.storeRipRelativeFloat32(r.reg);
         masm.append(AsmJSGlobalAccess(label, globalDataOffset));
+#elif defined(JS_CODEGEN_X86)
+        CodeOffset label = masm.vmovssWithPatch(r.reg, PatchedAbsoluteAddress());
+        masm.append(AsmJSGlobalAccess(label, globalDataOffset));
 #else
         MOZ_CRASH("BaseCompiler platform hook: storeGlobalVarF32");
 #endif
     }
 
     void storeGlobalVarF64(unsigned globalDataOffset, RegF64 r)
     {
-#ifdef JS_CODEGEN_X64
+#if defined(JS_CODEGEN_X64)
         CodeOffset label = masm.storeRipRelativeDouble(r.reg);
         masm.append(AsmJSGlobalAccess(label, globalDataOffset));
+#elif defined(JS_CODEGEN_X86)
+        CodeOffset label = masm.vmovsdWithPatch(r.reg, PatchedAbsoluteAddress());
+        masm.append(AsmJSGlobalAccess(label, globalDataOffset));
 #else
         MOZ_CRASH("BaseCompiler platform hook: storeGlobalVarF64");
 #endif
     }
 
     //////////////////////////////////////////////////////////////////////
     //
     // Heap access.
 
-#if defined(JS_CODEGEN_X64)
-    // Copied from CodeGenerator-x64.cpp
-    // TODO / CLEANUP - share with the code generator.
+    // TODO / CLEANUP - cloned from MIRGraph.cpp, should share.
+
+    bool needsBoundsCheckBranch(const MWasmMemoryAccess& access) const {
+        // A heap access needs a bounds-check branch if we're not relying on signal
+        // handlers to catch errors, and if it's not proven to be within bounds.
+        // We use signal-handlers on x64, but on x86 there isn't enough address
+        // space for a guard region.  Also, on x64 the atomic loads and stores
+        // can't (yet) use the signal handlers.
+
+#if defined(ASMJS_MAY_USE_SIGNAL_HANDLERS_FOR_OOB)
+        if (mg_.args.useSignalHandlersForOOB && !access.isAtomicAccess())
+            return false;
+#endif
+
+        return access.needsBoundsCheck();
+    }
+
+    bool faultOnOutOfBounds(const MWasmMemoryAccess& access) {
+        return access.isAtomicAccess() || !isCompilingAsmJS();
+    }
+
+    // For asm.js code only: If we have a non-zero offset, it's possible that
+    // |ptr| itself is out of bounds, while adding the offset computes an
+    // in-bounds address. To catch this case, we need a second branch, which we
+    // emit out of line since it's unlikely to be needed in normal programs.
+    // For this, we'll generate an OffsetBoundsCheck OOL stub.
+
+    bool needsOffsetBoundsCheck(const MWasmMemoryAccess& access) const {
+        return isCompilingAsmJS() && access.offset() != 0;
+    }
+
+#if defined(JS_CODEGEN_X86) || defined(JS_CODEGEN_X64)
+
+# if defined(JS_CODEGEN_X64)
+    // TODO / CLEANUP - copied from CodeGenerator-x64.cpp, should share.
 
     wasm::MemoryAccess
     AsmJSMemoryAccess(uint32_t before, wasm::MemoryAccess::OutOfBoundsBehavior throwBehavior,
                       uint32_t offsetWithinWholeSimdVector = 0)
     {
         return wasm::MemoryAccess(before, throwBehavior, wasm::MemoryAccess::WrapOffset,
                                   offsetWithinWholeSimdVector);
     }
-#endif
-
-    void memoryBarrier(MemoryBarrierBits barrier) {
-#if defined(JS_CODEGEN_X86) || defined(JS_CODEGEN_X64)
-        if (barrier & MembarStoreLoad)
-            masm.storeLoadFence();
-#else
-        MOZ_CRASH("BaseCompiler platform hook: memoryBarrier");
-#endif
-    }
-
-    // Cloned from MIRGraph.cpp, merge somehow?
-
-    bool needsBoundsCheckBranch(const MWasmMemoryAccess& access) const {
-        // A heap access needs a bounds-check branch if we're not relying on signal
-        // handlers to catch errors, and if it's not proven to be within bounds.
-        // We use signal-handlers on x64, but on x86 there isn't enough address
-        // space for a guard region.  Also, on x64 the atomic loads and stores
-        // can't (yet) use the signal handlers.
-
-#if defined(ASMJS_MAY_USE_SIGNAL_HANDLERS_FOR_OOB)
-        if (mg_.args.useSignalHandlersForOOB && !access.isAtomicAccess())
+# endif
+
+    class OffsetBoundsCheck : public OutOfLineCode
+    {
+        Label* maybeOutOfBounds;
+        Register ptrReg;
+        int32_t offset;
+
+      public:
+        OffsetBoundsCheck(Label* maybeOutOfBounds, Register ptrReg, int32_t offset)
+            : maybeOutOfBounds(maybeOutOfBounds),
+              ptrReg(ptrReg),
+              offset(offset)
+        {}
+
+        void generate(MacroAssembler& masm) {
+            // asm.js code only:
+            //
+            // The access is heap[ptr + offset]. The inline code checks that
+            // ptr < heap.length - offset. We get here when that fails. We need to check
+            // for the case where ptr + offset >= 0, in which case the access is still
+            // in bounds.
+
+            MOZ_ASSERT(offset != 0,
+                       "An access without a constant offset doesn't need a separate "
+                       "OffsetBoundsCheck");
+            masm.cmp32(ptrReg, Imm32(-uint32_t(offset)));
+            if (maybeOutOfBounds)
+                masm.j(Assembler::Below, maybeOutOfBounds);
+            else
+                masm.j(Assembler::Below, wasm::JumpTarget::OutOfBounds);
+
+# ifdef JS_CODEGEN_X64
+            // In order to get the offset to wrap properly, we must sign-extend the
+            // pointer to 32-bits. We'll zero out the sign extension immediately
+            // after the access to restore asm.js invariants.
+            masm.movslq(ptrReg, ptrReg);
+# endif
+
+            masm.jmp(rejoin());
+        }
+    };
+
+    // CodeGeneratorX86Shared::emitAsmJSBoundsCheckBranch()
+
+    MOZ_MUST_USE
+    bool emitBoundsCheckBranch(const MWasmMemoryAccess& access, RegI32 ptr, Label* maybeFail) {
+        Label* pass = nullptr;
+
+        if (needsOffsetBoundsCheck(access)) {
+            auto* oolCheck = new(alloc_) OffsetBoundsCheck(maybeFail, ptr.reg, access.offset());
+            maybeFail = oolCheck->entry();
+            pass = oolCheck->rejoin();
+            if (!addOutOfLineCode(oolCheck))
+                return false;
+        }
+
+        // The bounds check is a comparison with an immediate value. The asm.js
+        // module linking process will add the length of the heap to the immediate
+        // field, so -access->endOffset() will turn into
+        // (heapLength - access->endOffset()), allowing us to test whether the end
+        // of the access is beyond the end of the heap.
+        uint32_t cmpOffset = masm.cmp32WithPatch(ptr.reg, Imm32(-access.endOffset())).offset();
+        if (maybeFail)
+            masm.j(Assembler::Above, maybeFail);
+        else
+            masm.j(Assembler::Above, wasm::JumpTarget::OutOfBounds);
+
+        if (pass)
+            masm.bind(pass);
+
+        masm.append(wasm::BoundsCheck(cmpOffset));
+        return true;
+    }
+
+    class OutOfLineLoadTypedArrayOOB : public OutOfLineCode
+    {
+        Scalar::Type viewType;
+        AnyRegister dest;
+      public:
+        OutOfLineLoadTypedArrayOOB(Scalar::Type viewType, AnyRegister dest)
+            : viewType(viewType),
+              dest(dest)
+        {}
+
+        void generate(MacroAssembler& masm) {
+            switch (viewType) {
+              case Scalar::Float32x4:
+              case Scalar::Int32x4:
+              case Scalar::Int8x16:
+              case Scalar::Int16x8:
+              case Scalar::MaxTypedArrayViewType:
+                MOZ_CRASH("unexpected array type");
+              case Scalar::Float32:
+                masm.loadConstantFloat32(float(GenericNaN()), dest.fpu());
+                break;
+              case Scalar::Float64:
+                masm.loadConstantDouble(GenericNaN(), dest.fpu());
+                break;
+              case Scalar::Int8:
+              case Scalar::Uint8:
+              case Scalar::Int16:
+              case Scalar::Uint16:
+              case Scalar::Int32:
+              case Scalar::Uint32:
+              case Scalar::Uint8Clamped:
+                masm.movePtr(ImmWord(0), dest.gpr());
+                break;
+            }
+            masm.jump(rejoin());
+        }
+    };
+
+    MOZ_MUST_USE
+    bool maybeEmitLoadBoundsCheck(const MWasmMemoryAccess& access, RegI32 ptr, AnyRegister dest,
+                                  OutOfLineCode** ool)
+    {
+        *ool = nullptr;
+        if (!needsBoundsCheckBranch(access))
+            return true;
+
+        if (faultOnOutOfBounds(access))
+            return emitBoundsCheckBranch(access, ptr, nullptr);
+
+        // TODO / MEMORY: We'll allocate *a lot* of these OOL objects,
+        // thus risking OOM on a platform that is already
+        // memory-constrained.  We could opt to allocate this path
+        // in-line instead.
+        *ool = new (alloc_) OutOfLineLoadTypedArrayOOB(access.accessType(), dest);
+        if (!addOutOfLineCode(*ool))
             return false;
-#endif
-
-        return access.needsBoundsCheck();
-    }
-
-#if defined(JS_CODEGEN_X64) || defined(JS_CODEGEN_X86)
-    void verifyHeapAccessDisassembly(uint32_t before, uint32_t after, bool isLoad,
-                                     Scalar::Type accessType, int nelems, Operand srcAddr,
-                                     AnyReg dest)
-    {
-#ifdef DEBUG
-        // TODO / MISSING: this needs to be adapted from what's in the
-        // platform's CodeGenerator; that code takes an LAllocation as
-        // the last arg now.
-#endif
-    }
-#endif
-
-    void loadHeap(const MWasmMemoryAccess& access, RegI32 ptr, AnyReg dest) {
-#if defined(JS_CODEGEN_X64)
-        // CodeGeneratorX64::visitAsmJSLoadHeap()
-
-        if (needsBoundsCheckBranch(access))
-            MOZ_CRASH("BaseCompiler platform hook: bounds checking");
-
+
+        return emitBoundsCheckBranch(access, ptr, (*ool)->entry());
+    }
+
+    MOZ_MUST_USE
+    bool maybeEmitStoreBoundsCheck(const MWasmMemoryAccess& access, RegI32 ptr, Label** rejoin) {
+        *rejoin = nullptr;
+        if (!needsBoundsCheckBranch(access))
+            return true;
+
+        if (faultOnOutOfBounds(access))
+            return emitBoundsCheckBranch(access, ptr, nullptr);
+
+        *rejoin = newLabel();
+        if (!*rejoin)
+            return false;
+
+        return emitBoundsCheckBranch(access, ptr, *rejoin);
+    }
+
+    void cleanupAfterBoundsCheck(const MWasmMemoryAccess& access, RegI32 ptr) {
+# ifdef JS_CODEGEN_X64
+        if (needsOffsetBoundsCheck(access)) {
+            // Zero out the high 32 bits, in case the OffsetBoundsCheck code had to
+            // sign-extend (movslq) the pointer value to get wraparound to work.
+            masm.movl(ptr.reg, ptr.reg);
+        }
+# endif
+    }
+
+    MOZ_MUST_USE
+    bool loadHeap(const MWasmMemoryAccess& access, RegI32 ptr, AnyReg dest) {
+        OutOfLineCode* ool = nullptr;
+        if (!maybeEmitLoadBoundsCheck(access, ptr, dest.any(), &ool))
+            return false;
+
+# if defined(JS_CODEGEN_X64)
         Operand srcAddr(HeapReg, ptr.reg, TimesOne, access.offset());
 
         uint32_t before = masm.size();
         switch (access.accessType()) {
           case Scalar::Int8:      masm.movsbl(srcAddr, dest.i32().reg); break;
           case Scalar::Uint8:     masm.movzbl(srcAddr, dest.i32().reg); break;
           case Scalar::Int16:     masm.movswl(srcAddr, dest.i32().reg); break;
           case Scalar::Uint16:    masm.movzwl(srcAddr, dest.i32().reg); break;
           case Scalar::Int32:
           case Scalar::Uint32:    masm.movl(srcAddr, dest.i32().reg); break;
           case Scalar::Float32:   masm.loadFloat32(srcAddr, dest.f32().reg); break;
           case Scalar::Float64:   masm.loadDouble(srcAddr, dest.f64().reg); break;
           default:
             MOZ_CRASH("Compiler bug: Unexpected array type");
         }
+
+        masm.append(AsmJSMemoryAccess(before, wasm::MemoryAccess::CarryOn));
+# elif defined(JS_CODEGEN_X86)
+        Operand srcAddr(ptr.reg, access.offset());
+
+        bool mustMove = false;
+        switch (access.accessType()) {
+          case Scalar::Int8:
+          case Scalar::Uint8: {
+            Register rd = dest.i32().reg;
+            Register rt = rd;
+            if (!SingleByteRegs.has(rd)) {
+                mustMove = true;
+                rt = ScratchRegX86;
+            }
+            if (access.accessType() == Scalar::Int8)
+                masm.movsblWithPatch(srcAddr, rt);
+            else
+                masm.movzblWithPatch(srcAddr, rt);
+            break;
+          }
+          case Scalar::Int16:     masm.movswlWithPatch(srcAddr, dest.i32().reg); break;
+          case Scalar::Uint16:    masm.movzwlWithPatch(srcAddr, dest.i32().reg); break;
+          case Scalar::Int32:
+          case Scalar::Uint32:    masm.movlWithPatch(srcAddr, dest.i32().reg); break;
+          case Scalar::Float32:   masm.vmovssWithPatch(srcAddr, dest.f32().reg); break;
+          case Scalar::Float64:   masm.vmovsdWithPatch(srcAddr, dest.f64().reg); break;
+          default:
+            MOZ_CRASH("Compiler bug: Unexpected array type");
+        }
         uint32_t after = masm.size();
-
-        masm.append(AsmJSMemoryAccess(before, wasm::MemoryAccess::CarryOn));
-        verifyHeapAccessDisassembly(before, after, IsLoad(true), access.accessType(), 0, srcAddr, dest);
-#else
-        MOZ_CRASH("BaseCompiler platform hook: loadHeap");
-#endif
-    }
-
-    void storeHeap(const MWasmMemoryAccess& access, RegI32 ptr, AnyReg src) {
-#if defined(JS_CODEGEN_X64)
-        // CodeGeneratorX64::visitAsmJSStoreHeap()
-
-        if (needsBoundsCheckBranch(access))
-            MOZ_CRASH("BaseCompiler platform hook: bounds checking");
-
+        if (mustMove)
+            masm.mov(ScratchRegX86, dest.i32().reg);
+
+        masm.append(wasm::MemoryAccess(after));
+# else
+        MOZ_CRASH("Compiler bug: Unexpected platform.");
+# endif
+
+        if (ool) {
+            cleanupAfterBoundsCheck(access, ptr);
+            masm.bind(ool->rejoin());
+        }
+        return true;
+    }
+
+    MOZ_MUST_USE
+    bool storeHeap(const MWasmMemoryAccess& access, RegI32 ptr, AnyReg src) {
+        Label* rejoin = nullptr;
+        if (!maybeEmitStoreBoundsCheck(access, ptr, &rejoin))
+            return false;
+
+# if defined(JS_CODEGEN_X64)
         Operand dstAddr(HeapReg, ptr.reg, TimesOne, access.offset());
 
         uint32_t before = masm.size();
         switch (access.accessType()) {
           case Scalar::Int8:
           case Scalar::Uint8:        masm.movb(src.i32().reg, dstAddr); break;
           case Scalar::Int16:
           case Scalar::Uint16:       masm.movw(src.i32().reg, dstAddr); break;
           case Scalar::Int32:
           case Scalar::Uint32:       masm.movl(src.i32().reg, dstAddr); break;
           case Scalar::Float32:      masm.storeFloat32(src.f32().reg, dstAddr); break;
           case Scalar::Float64:      masm.storeDouble(src.f64().reg, dstAddr); break;
           default:
               MOZ_CRASH("Compiler bug: Unexpected array type");
         }
+
+        masm.append(AsmJSMemoryAccess(before, wasm::MemoryAccess::CarryOn));
+# elif defined(JS_CODEGEN_X86)
+        Operand dstAddr(ptr.reg, access.offset());
+
+        bool didMove = false;
+        if (access.byteSize() == 1 && !SingleByteRegs.has(src.i32().reg)) {
+            didMove = true;
+            masm.mov(src.i32().reg, ScratchRegX86);
+        }
+        switch (access.accessType()) {
+          case Scalar::Int8:
+          case Scalar::Uint8: {
+            Register rs = src.i32().reg;
+            Register rt = didMove ? ScratchRegX86 : rs;
+            masm.movbWithPatch(rt, dstAddr);
+            break;
+          }
+          case Scalar::Int16:
+          case Scalar::Uint16:       masm.movwWithPatch(src.i32().reg, dstAddr); break;
+          case Scalar::Int32:
+          case Scalar::Uint32:       masm.movlWithPatch(src.i32().reg, dstAddr); break;
+          case Scalar::Float32:      masm.vmovssWithPatch(src.f32().reg, dstAddr); break;
+          case Scalar::Float64:      masm.vmovsdWithPatch(src.f64().reg, dstAddr); break;
+          default:
+              MOZ_CRASH("Compiler bug: Unexpected array type");
+        }
         uint32_t after = masm.size();
 
-        masm.append(AsmJSMemoryAccess(before, wasm::MemoryAccess::CarryOn));
-        verifyHeapAccessDisassembly(before, after, IsLoad(false), access.accessType(), 0, dstAddr, src);
+        masm.append(wasm::MemoryAccess(after));
+# else
+        MOZ_CRASH("Compiler bug: unexpected platform");
+# endif
+
+        if (rejoin) {
+            cleanupAfterBoundsCheck(access, ptr);
+            masm.bind(rejoin);
+        }
+        return true;
+    }
+
 #else
+
+    MOZ_MUST_USE
+    bool loadHeap(const MWasmMemoryAccess& access, RegI32 ptr, AnyReg dest) {
+        MOZ_CRASH("BaseCompiler platform hook: loadHeap");
+    }
+
+    MOZ_MUST_USE
+    bool storeHeap(const MWasmMemoryAccess& access, RegI32 ptr, AnyReg src) {
         MOZ_CRASH("BaseCompiler platform hook: storeHeap");
+    }
+
 #endif
-    }
 
     ////////////////////////////////////////////////////////////
 
     // Generally speaking, ABOVE this point there should be no value
     // stack manipulation (calls to popI32 etc).
 
     // Generally speaking, BELOW this point there should be no
     // platform dependencies.  We make an exception for x86 register
@@ -5211,35 +5498,38 @@ BaseCompiler::emitLoad(ValType type, Sca
     // below the minimum heap length.
 
     MWasmMemoryAccess access(viewType, addr.align);
     access.setOffset(addr.offset);
 
     switch (type) {
       case ValType::I32: {
         RegI32 rp = popI32();
-        loadHeap(access, rp, AnyReg(rp));
+        if (!loadHeap(access, rp, AnyReg(rp)))
+            return false;
         pushI32(rp);
         break;
       }
       case ValType::I64:
         MOZ_CRASH("Unimplemented: loadHeap i64");
         break;
       case ValType::F32: {
         RegI32 rp = popI32();
         RegF32 rv = needF32();
-        loadHeap(access, rp, AnyReg(rv));
+        if (!loadHeap(access, rp, AnyReg(rv)))
+            return false;
         pushF32(rv);
         freeI32(rp);
         break;
       }
       case ValType::F64: {
         RegI32 rp = popI32();
         RegF64 rv = needF64();
-        loadHeap(access, rp, AnyReg(rv));
+        if (!loadHeap(access, rp, AnyReg(rv)))
+            return false;
         pushF64(rv);
         freeI32(rp);
         break;
       }
       default:
         MOZ_CRASH("loadHeap type");
         break;
     }
@@ -5262,36 +5552,39 @@ BaseCompiler::emitStore(ValType resultTy
 
     MWasmMemoryAccess access(viewType, addr.align);
     access.setOffset(addr.offset);
 
     switch (resultType) {
       case ValType::I32: {
         RegI32 rp, rv;
         pop2xI32(&rp, &rv);
-        storeHeap(access, rp, AnyReg(rv));
+        if (!storeHeap(access, rp, AnyReg(rv)))
+            return false;
         freeI32(rp);
         pushI32(rv);
         break;
       }
       case ValType::I64:
         MOZ_CRASH("Unimplemented: storeHeap i64");
         break;
       case ValType::F32: {
         RegF32 rv = popF32();
         RegI32 rp = popI32();
-        storeHeap(access, rp, AnyReg(rv));
+        if (!storeHeap(access, rp, AnyReg(rv)))
+            return false;
         freeI32(rp);
         pushF32(rv);
         break;
       }
       case ValType::F64: {
         RegF64 rv = popF64();
         RegI32 rp = popI32();
-        storeHeap(access, rp, AnyReg(rv));
+        if (!storeHeap(access, rp, AnyReg(rv)))
+            return false;
         freeI32(rp);
         pushF64(rv);
         break;
       }
       default:
         MOZ_CRASH("storeHeap type");
         break;
     }
@@ -5543,27 +5836,29 @@ BaseCompiler::emitStoreWithCoercion(ValT
     MWasmMemoryAccess access(viewType, addr.align);
     access.setOffset(addr.offset);
 
     if (resultType == ValType::F32 && viewType == Scalar::Float64) {
         RegF32 rv = popF32();
         RegF64 rw = needF64();
         masm.convertFloat32ToDouble(rv.reg, rw.reg);
         RegI32 rp = popI32();
-        storeHeap(access, rp, AnyReg(rw));
+        if (!storeHeap(access, rp, AnyReg(rw)))
+            return false;
         pushF32(rv);
         freeI32(rp);
         freeF64(rw);
     }
     else if (resultType == ValType::F64 && viewType == Scalar::Float32) {
         RegF64 rv = popF64();
         RegF32 rw = needF32();
         masm.convertDoubleToFloat32(rv.reg, rw.reg);
         RegI32 rp = popI32();
-        storeHeap(access, rp, AnyReg(rw));
+        if (!storeHeap(access, rp, AnyReg(rw)))
+            return false;
         pushF64(rv);
         freeI32(rp);
         freeF32(rw);
     }
     else
         MOZ_CRASH("unexpected coerced store");
 
     return true;
@@ -6358,17 +6653,17 @@ volatileReturnGPR()
 LiveRegisterSet BaseCompiler::VolatileReturnGPR = volatileReturnGPR();
 
 } // wasm
 } // js
 
 bool
 js::wasm::BaselineCanCompile(const FunctionGenerator* fg)
 {
-#if defined(JS_CODEGEN_X64)
+#if defined(JS_CODEGEN_X64) || defined(JS_CODEGEN_X86)
     if (!fg->usesSignalsForInterrupts())
         return false;
 
     if (fg->usesAtomics())
         return false;
 
     if (fg->usesSimd())
         return false;
