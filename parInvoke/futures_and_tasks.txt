June 23, 2014.


[This is seriously half-baked but there are some good ideas here, such
 as Multicore.build being a /slicing/ primitive that makes use of an
 underlying task system to execute tasks across the slices.]


= Tasks =

Consider the notion of a task and a global, implicit task queue.
Tasks are not expressed objects and there's no way to wait on them.

Multicore.addTask(task-thunk, [collector, name])

  Add a task, represented by task-thunk, to the task queue.  It is not
  run at this point (unless the tasker is already running).

  If collector is not undefined then it is any object that will
  receive the return value when the tasks are all done, though not
  before, and the property name in the object that receives it is
  "name".  Value reception is race-free.  If there are conflicts then
  the value at name will be the undefined value (why not?).

  Importantly task-thunk can call Multicore.add.

  Tasks are run concurrently until one bails out for violating safety
  constraints.  (Guarantee of concurrency.)  At that point, the
  violating task will be run by itself.

  Tasks are run in order, to the extent that is observable.
  (Guarantee of determinism.)  Tasks will not block.

  Tasks, once completed, are not re-run unless an earlier tasks bails
  out for violating safety constraints.  (Guarantee of progress.)

Multicore.pushQueue()

  Push an implicit queue onto which tasks will be added.

Multicore.runTasks()

  If the current queue is the topmost queue and the tasker is not
  running, then the tasker is started.  Otherwise nothing happens.
  The call to runTasks returns when the task queue is empty.

  If the current queue is not the topmost queue then the tasks on the
  topmost queue are collected into one meta-task that will be complete
  when the subtasks are complete; the queue stack is popped; and the
  meta-task is enqueued on the queue above.  The call to runTasks then
  returns.

  runTasks() returns true in the first case, false otherwise.

Example:

  var result={};
  var id=0;
  for ( var y=0 ; y < rows ; y += tileysize )
    for ( var x=0 ; x < cols ; x += tilexsize )
      Multicore.add(function() {
                      var sum=0;
                      for ( var j=0 ; j < tileysize ; j++ )
                        for ( var i=0 ; i < tilexsize ; i++ )
                          sum += grid[y+j][x+i];
                      return sum;
                    },
                    result,
                    id++);
  Multicore.run();

Critique:

  On the positive side:

  Tasks can be of variable complexity, and we can add tasks in
  response to needs of subproblems.  So long as computations are
  side-effect free we have flexible concurrency.

  Within this framework the most useful view of Multicore.build() is
  that it is /the/ primitive way to slice up a volume and turn
  computations on the resulting subvolumes into tasks.  (With an
  additional parameter, build can be made to not start the tasker.)

  On the negative side:

  There really is no way to wait for a set of tasks to be done, only a
  way to wait for all tasks to be done.  That's probably OK for a
  program that sets up a computation in various stages and runs it,
  but it requires that everyone is aware of what's going on and canned
  routines are hard to create.  Pipelines become impossible without a
  return to sequential code.

  Explicit queues would solve the problem but would create deadlock
  issues.

  Alternatively perhaps there's some notion of a "metatask", a stack of
  implicit task queues:

    Multicore.start()
    ... add tasks ...
    Multicore.run()

  Now if the tasks want to add more tasks, they must encapsulate those
  in a start .. run pair if they want to wait on them, or they can
  just add them to the current queue (the topmost queue by default) if
  they just need to be run.


= Futures =

Consider Multicore.future(), which takes a thunk and returns a
suspended parallel computation:

  var f = Multicore.future(() => something)

Now f.resolve runs it and returns its result:

  var result = f.resolve();

This only becomes interesting when there are multiple futures queued
up because (a) the futures can be run concurrently and (b) we may
optimize in the engine so as not to have to leave and re-enter the
parallel engine for every future:

  var fs = inputs.map((v) => Multicore.future(() => something with v));
  var results = fs.map((f) => f.resolve())

In other words, we've stepped from serial data-parallel mode to some
quasi-task-parallel mode where each task is a data-parallel section.

There are important restrictions:

 * Parallel computation may run only when the main thread is blocked
   in a call to resolve().

 * If one parallel computation falls back to sequential it must run by
   itself; the others must be suspended.  (This means warmup sections
   don't overlap.)

That said, (a) multiple parallel sections that all run in parallel
mode can run concurrently, and (b) parallel sections don't interfere
with each other in the sense that an abort in one causes an abort in
another.

Observe that Multicore.build() is probably "just" a combination of
efficient storage management (array neutering and transfer, subvolume
and cursor creation, and subvolume zeroing) with Multicore.future().

That said, what Multicore.build() brings to the table is a safe
partitioning of an output volume.  With only Multicore.future() it's
not clear we could make that work so easily.  So futures may be a
complementary idea, sharing a lot of infrastructure with build().

Perhaps there is a futureBuild() function that creates a future that
performs a build as part of a set of futures?

*** Note that futures can be interdependent and in fact cyclically
    dependent because they have identity and there are individual
    resolve methods.





In turn, Multicore.future() and future.resolve() are probably special
cases of a more general task-parallel idea:

  inputs.map((v) => Multicore.add(() => something with v))
  var results = Multicore.run()

Crucially, though, the task thunk can add further tasks.  Consider a
2D case:

  inputs.map((v) => Multicore.add(() => v.map((w) => Multicore.add(() => something with w))))
  var results = Multicore.run()

Sorting out the results is a little work but we can probably make the
provision that they are delivered in the order the tasks were added.

Note also that this does not capture future.resolve() properly: there
is only one run queue, and unclear what a nested call to run() would
do.


A final piece of the puzzle is coordination among tasks.  In the
previous model all tasks run to completion, independently.  There is 
